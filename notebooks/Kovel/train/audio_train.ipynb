{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"luli0034/music-tags-to-spectrogram\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 771\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "subset_size = int(len(ds) * 0.5)\n",
    "subset = ds.select(range(subset_size))\n",
    "print(subset)\n",
    "ds = subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, ds, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_frame = ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            genres = self.data_frame[index][\"text\"]\n",
    "            if self.transform:\n",
    "                image = self.transform(self.data_frame[index][\"image\"])\n",
    "            return image, genres\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return np.ones((256, 256, 3)), self.data_frame[index][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_collate(batch):\n",
    "#     batch = list(filter(lambda x: x is not None, batch))\n",
    "#     return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(dataloader, model):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            features.append(output.cpu().numpy())\n",
    "    return np.vstack(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поэтому я превращу их в квадрат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MusicDataset(ds_train, transform=image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MusicDataset(ds_test, transform=image_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Identity()\n",
    "model.to(device) \n",
    "resnet = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = ds_train.remove_columns('image')\n",
    "all_genres_test = ds_test.remove_columns('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = [genre[\"text\"].split(\" \") for genre in all_genres]\n",
    "all_genres_test = [genre[\"text\"].split(\" \")  for genre in all_genres_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atmospheric', 'ambient', 'darkambient']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genres[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['easylistening', 'soundtrack', 'ambient', 'chillout']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genres_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['darkwave', 'oriental'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(all_genres)\n",
    "y_test_encoder = mlb.transform(all_genres_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['60s', '70s', '80s', '90s', 'accordion', 'acidjazz',\n",
       "       'acousticbassguitar', 'acousticguitar', 'african', 'alternative',\n",
       "       'alternativerock', 'ambient', 'atmospheric', 'bass', 'beat',\n",
       "       'blues', 'bossanova', 'brass', 'breakbeat', 'cello', 'celtic',\n",
       "       'chanson', 'chillout', 'choir', 'clarinet', 'classical',\n",
       "       'classicalguitar', 'club', 'computer', 'contemporary', 'country',\n",
       "       'dance', 'darkambient', 'deephouse', 'doublebass', 'downtempo',\n",
       "       'drummachine', 'drumnbass', 'drums', 'dubstep', 'easylistening',\n",
       "       'edm', 'electricguitar', 'electricpiano', 'electronic',\n",
       "       'electronica', 'electropop', 'ethno', 'eurodance', 'experimental',\n",
       "       'flute', 'folk', 'funk', 'fusion', 'gothic', 'grunge', 'guitar',\n",
       "       'hard', 'hardrock', 'harp', 'hiphop', 'house', 'improvisation',\n",
       "       'indie', 'industrial', 'instrumentalpop', 'instrumentalrock',\n",
       "       'jazz', 'jazzfunk', 'keyboard', 'latin', 'lounge', 'medieval',\n",
       "       'metal', 'minimal', 'newage', 'newwave', 'orchestra', 'orchestral',\n",
       "       'pad', 'percussion', 'piano', 'pipeorgan', 'pop', 'popfolk',\n",
       "       'poprock', 'postrock', 'progressive', 'psychedelic', 'punkrock',\n",
       "       'rap', 'reggae', 'rhodes', 'rnb', 'rock', 'rocknroll', 'sampler',\n",
       "       'saxophone', 'singersongwriter', 'ska', 'soul', 'soundtrack',\n",
       "       'strings', 'swing', 'symphonic', 'synthesizer', 'synthpop',\n",
       "       'techno', 'trance', 'tribal', 'triphop', 'trombone', 'trumpet',\n",
       "       'ukulele', 'viola', 'violin', 'voice', 'world', 'worldfusion'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = extract_image_features(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor = torch.tensor(train_features, dtype=torch.float32).to(device)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_size=feature_tensor.shape[1], num_classes=labels_tensor.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline CV Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 /100], Loss: 0.6932\n",
      "Epoch [2 /100], Loss: 0.6809\n",
      "Epoch [3 /100], Loss: 0.6692\n",
      "Epoch [4 /100], Loss: 0.6583\n",
      "Epoch [5 /100], Loss: 0.6479\n",
      "Epoch [6 /100], Loss: 0.6382\n",
      "Epoch [7 /100], Loss: 0.6290\n",
      "Epoch [8 /100], Loss: 0.6204\n",
      "Epoch [9 /100], Loss: 0.6122\n",
      "Epoch [10 /100], Loss: 0.6045\n",
      "Epoch [11 /100], Loss: 0.5972\n",
      "Epoch [12 /100], Loss: 0.5903\n",
      "Epoch [13 /100], Loss: 0.5838\n",
      "Epoch [14 /100], Loss: 0.5777\n",
      "Epoch [15 /100], Loss: 0.5719\n",
      "Epoch [16 /100], Loss: 0.5664\n",
      "Epoch [17 /100], Loss: 0.5612\n",
      "Epoch [18 /100], Loss: 0.5563\n",
      "Epoch [19 /100], Loss: 0.5516\n",
      "Epoch [20 /100], Loss: 0.5472\n",
      "Epoch [21 /100], Loss: 0.5430\n",
      "Epoch [22 /100], Loss: 0.5391\n",
      "Epoch [23 /100], Loss: 0.5353\n",
      "Epoch [24 /100], Loss: 0.5317\n",
      "Epoch [25 /100], Loss: 0.5283\n",
      "Epoch [26 /100], Loss: 0.5251\n",
      "Epoch [27 /100], Loss: 0.5220\n",
      "Epoch [28 /100], Loss: 0.5190\n",
      "Epoch [29 /100], Loss: 0.5162\n",
      "Epoch [30 /100], Loss: 0.5136\n",
      "Epoch [31 /100], Loss: 0.5110\n",
      "Epoch [32 /100], Loss: 0.5086\n",
      "Epoch [33 /100], Loss: 0.5063\n",
      "Epoch [34 /100], Loss: 0.5041\n",
      "Epoch [35 /100], Loss: 0.5020\n",
      "Epoch [36 /100], Loss: 0.5000\n",
      "Epoch [37 /100], Loss: 0.4980\n",
      "Epoch [38 /100], Loss: 0.4962\n",
      "Epoch [39 /100], Loss: 0.4944\n",
      "Epoch [40 /100], Loss: 0.4927\n",
      "Epoch [41 /100], Loss: 0.4911\n",
      "Epoch [42 /100], Loss: 0.4896\n",
      "Epoch [43 /100], Loss: 0.4881\n",
      "Epoch [44 /100], Loss: 0.4866\n",
      "Epoch [45 /100], Loss: 0.4853\n",
      "Epoch [46 /100], Loss: 0.4839\n",
      "Epoch [47 /100], Loss: 0.4827\n",
      "Epoch [48 /100], Loss: 0.4815\n",
      "Epoch [49 /100], Loss: 0.4803\n",
      "Epoch [50 /100], Loss: 0.4792\n",
      "Epoch [51 /100], Loss: 0.4781\n",
      "Epoch [52 /100], Loss: 0.4770\n",
      "Epoch [53 /100], Loss: 0.4760\n",
      "Epoch [54 /100], Loss: 0.4751\n",
      "Epoch [55 /100], Loss: 0.4741\n",
      "Epoch [56 /100], Loss: 0.4732\n",
      "Epoch [57 /100], Loss: 0.4724\n",
      "Epoch [58 /100], Loss: 0.4715\n",
      "Epoch [59 /100], Loss: 0.4707\n",
      "Epoch [60 /100], Loss: 0.4700\n",
      "Epoch [61 /100], Loss: 0.4692\n",
      "Epoch [62 /100], Loss: 0.4685\n",
      "Epoch [63 /100], Loss: 0.4678\n",
      "Epoch [64 /100], Loss: 0.4671\n",
      "Epoch [65 /100], Loss: 0.4664\n",
      "Epoch [66 /100], Loss: 0.4658\n",
      "Epoch [67 /100], Loss: 0.4652\n",
      "Epoch [68 /100], Loss: 0.4646\n",
      "Epoch [69 /100], Loss: 0.4640\n",
      "Epoch [70 /100], Loss: 0.4635\n",
      "Epoch [71 /100], Loss: 0.4629\n",
      "Epoch [72 /100], Loss: 0.4624\n",
      "Epoch [73 /100], Loss: 0.4619\n",
      "Epoch [74 /100], Loss: 0.4614\n",
      "Epoch [75 /100], Loss: 0.4609\n",
      "Epoch [76 /100], Loss: 0.4604\n",
      "Epoch [77 /100], Loss: 0.4600\n",
      "Epoch [78 /100], Loss: 0.4595\n",
      "Epoch [79 /100], Loss: 0.4591\n",
      "Epoch [80 /100], Loss: 0.4587\n",
      "Epoch [81 /100], Loss: 0.4583\n",
      "Epoch [82 /100], Loss: 0.4579\n",
      "Epoch [83 /100], Loss: 0.4575\n",
      "Epoch [84 /100], Loss: 0.4571\n",
      "Epoch [85 /100], Loss: 0.4568\n",
      "Epoch [86 /100], Loss: 0.4564\n",
      "Epoch [87 /100], Loss: 0.4561\n",
      "Epoch [88 /100], Loss: 0.4558\n",
      "Epoch [89 /100], Loss: 0.4554\n",
      "Epoch [90 /100], Loss: 0.4551\n",
      "Epoch [91 /100], Loss: 0.4548\n",
      "Epoch [92 /100], Loss: 0.4545\n",
      "Epoch [93 /100], Loss: 0.4542\n",
      "Epoch [94 /100], Loss: 0.4539\n",
      "Epoch [95 /100], Loss: 0.4537\n",
      "Epoch [96 /100], Loss: 0.4534\n",
      "Epoch [97 /100], Loss: 0.4531\n",
      "Epoch [98 /100], Loss: 0.4529\n",
      "Epoch [99 /100], Loss: 0.4526\n",
      "Epoch [100 /100], Loss: 0.4524\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(feature_tensor)\n",
    "    loss = criterion(outputs, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1} /100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = extract_image_features(test_loader, resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor_test = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
    "labels_tensor_test = torch.tensor(y_test_encoder, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_ouptputs = model(feature_tensor_test)\n",
    "    test_predictions = torch.sigmoid(test_ouptputs).cpu().numpy()\n",
    "    test_predictions = (test_predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.77      1.00      0.87       119\n",
      "           0       0.00      0.00      0.00         4\n",
      "           6       0.00      0.00      0.00         0\n",
      "           7       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         2\n",
      "           9       0.00      0.00      0.00         2\n",
      "           a       0.77      1.00      0.87       120\n",
      "           b       0.00      0.00      0.00        36\n",
      "           c       0.69      1.00      0.82       107\n",
      "           d       1.00      0.02      0.03        66\n",
      "           e       0.68      1.00      0.81       105\n",
      "           f       0.00      0.00      0.00        17\n",
      "           g       0.00      0.00      0.00        41\n",
      "           h       0.00      0.00      0.00        60\n",
      "           i       0.75      1.00      0.86       117\n",
      "           j       0.00      0.00      0.00         2\n",
      "           k       0.00      0.00      0.00        64\n",
      "           l       0.67      1.00      0.80       104\n",
      "           m       0.00      0.00      0.00        51\n",
      "           n       0.83      1.00      0.90       128\n",
      "           o       0.84      1.00      0.91       130\n",
      "           p       0.00      0.00      0.00        72\n",
      "           r       0.75      1.00      0.86       116\n",
      "           s       0.61      1.00      0.76        95\n",
      "           t       0.76      1.00      0.86       118\n",
      "           u       0.53      0.58      0.56        86\n",
      "           v       0.00      0.00      0.00        18\n",
      "           w       0.00      0.00      0.00        17\n",
      "           x       0.00      0.00      0.00        10\n",
      "           y       0.00      0.00      0.00        30\n",
      "           z       0.00      0.00      0.00        13\n",
      "\n",
      "   micro avg       0.73      0.71      0.72      1850\n",
      "   macro avg       0.31      0.37      0.32      1850\n",
      "weighted avg       0.57      0.71      0.61      1850\n",
      " samples avg       0.73      0.71      0.70      1850\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoder, test_predictions, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"multiclass_model_simple.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
