{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"luli0034/music-tags-to-spectrogram\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Возьму только 10 процентов датасета, чтобы просто проверить, что модель может обучиться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, ds, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_frame = ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            genres = self.data_frame[index][\"text\"]\n",
    "            if self.transform:\n",
    "                image = self.transform(self.data_frame[index][\"image\"])\n",
    "            return image, genres\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # return np.ones((256, 256, 3)), [\"classical\"] # TODO: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    batch = list(filter(lambda x: x is not None, batch))\n",
    "    return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(dataloader, model):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            features.append(output.cpu().numpy())\n",
    "    return np.vstack(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поэтому я превращу их в квадрат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MusicDataset(ds_train, transform=image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MusicDataset(ds_test, transform=image_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Identity()\n",
    "model.to(device) \n",
    "resnet = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = ds_train.remove_columns('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres_list = [genre['text'] for genre in all_genres]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) [np.int64(1)] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(all_genres)\n",
    "y_test_encoder = mlb.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrecognized data stream contents when reading image file\n"
     ]
    }
   ],
   "source": [
    "train_features = extract_image_features(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor = torch.tensor(train_features, dtype=torch.float32).to(device)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_size=feature_tensor.shape[1], num_classes=labels_tensor.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline CV Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 /100], Loss: 0.7226\n",
      "Epoch [2 /100], Loss: 0.7077\n",
      "Epoch [3 /100], Loss: 0.6937\n",
      "Epoch [4 /100], Loss: 0.6805\n",
      "Epoch [5 /100], Loss: 0.6682\n",
      "Epoch [6 /100], Loss: 0.6566\n",
      "Epoch [7 /100], Loss: 0.6457\n",
      "Epoch [8 /100], Loss: 0.6355\n",
      "Epoch [9 /100], Loss: 0.6258\n",
      "Epoch [10 /100], Loss: 0.6168\n",
      "Epoch [11 /100], Loss: 0.6083\n",
      "Epoch [12 /100], Loss: 0.6004\n",
      "Epoch [13 /100], Loss: 0.5929\n",
      "Epoch [14 /100], Loss: 0.5858\n",
      "Epoch [15 /100], Loss: 0.5792\n",
      "Epoch [16 /100], Loss: 0.5730\n",
      "Epoch [17 /100], Loss: 0.5671\n",
      "Epoch [18 /100], Loss: 0.5616\n",
      "Epoch [19 /100], Loss: 0.5563\n",
      "Epoch [20 /100], Loss: 0.5514\n",
      "Epoch [21 /100], Loss: 0.5468\n",
      "Epoch [22 /100], Loss: 0.5424\n",
      "Epoch [23 /100], Loss: 0.5382\n",
      "Epoch [24 /100], Loss: 0.5343\n",
      "Epoch [25 /100], Loss: 0.5306\n",
      "Epoch [26 /100], Loss: 0.5271\n",
      "Epoch [27 /100], Loss: 0.5238\n",
      "Epoch [28 /100], Loss: 0.5206\n",
      "Epoch [29 /100], Loss: 0.5176\n",
      "Epoch [30 /100], Loss: 0.5148\n",
      "Epoch [31 /100], Loss: 0.5121\n",
      "Epoch [32 /100], Loss: 0.5096\n",
      "Epoch [33 /100], Loss: 0.5071\n",
      "Epoch [34 /100], Loss: 0.5048\n",
      "Epoch [35 /100], Loss: 0.5026\n",
      "Epoch [36 /100], Loss: 0.5005\n",
      "Epoch [37 /100], Loss: 0.4986\n",
      "Epoch [38 /100], Loss: 0.4967\n",
      "Epoch [39 /100], Loss: 0.4948\n",
      "Epoch [40 /100], Loss: 0.4931\n",
      "Epoch [41 /100], Loss: 0.4915\n",
      "Epoch [42 /100], Loss: 0.4899\n",
      "Epoch [43 /100], Loss: 0.4884\n",
      "Epoch [44 /100], Loss: 0.4870\n",
      "Epoch [45 /100], Loss: 0.4856\n",
      "Epoch [46 /100], Loss: 0.4843\n",
      "Epoch [47 /100], Loss: 0.4830\n",
      "Epoch [48 /100], Loss: 0.4818\n",
      "Epoch [49 /100], Loss: 0.4806\n",
      "Epoch [50 /100], Loss: 0.4795\n",
      "Epoch [51 /100], Loss: 0.4785\n",
      "Epoch [52 /100], Loss: 0.4774\n",
      "Epoch [53 /100], Loss: 0.4765\n",
      "Epoch [54 /100], Loss: 0.4755\n",
      "Epoch [55 /100], Loss: 0.4746\n",
      "Epoch [56 /100], Loss: 0.4737\n",
      "Epoch [57 /100], Loss: 0.4729\n",
      "Epoch [58 /100], Loss: 0.4721\n",
      "Epoch [59 /100], Loss: 0.4713\n",
      "Epoch [60 /100], Loss: 0.4706\n",
      "Epoch [61 /100], Loss: 0.4699\n",
      "Epoch [62 /100], Loss: 0.4692\n",
      "Epoch [63 /100], Loss: 0.4685\n",
      "Epoch [64 /100], Loss: 0.4679\n",
      "Epoch [65 /100], Loss: 0.4672\n",
      "Epoch [66 /100], Loss: 0.4666\n",
      "Epoch [67 /100], Loss: 0.4661\n",
      "Epoch [68 /100], Loss: 0.4655\n",
      "Epoch [69 /100], Loss: 0.4650\n",
      "Epoch [70 /100], Loss: 0.4644\n",
      "Epoch [71 /100], Loss: 0.4639\n",
      "Epoch [72 /100], Loss: 0.4635\n",
      "Epoch [73 /100], Loss: 0.4630\n",
      "Epoch [74 /100], Loss: 0.4625\n",
      "Epoch [75 /100], Loss: 0.4621\n",
      "Epoch [76 /100], Loss: 0.4617\n",
      "Epoch [77 /100], Loss: 0.4612\n",
      "Epoch [78 /100], Loss: 0.4608\n",
      "Epoch [79 /100], Loss: 0.4605\n",
      "Epoch [80 /100], Loss: 0.4601\n",
      "Epoch [81 /100], Loss: 0.4597\n",
      "Epoch [82 /100], Loss: 0.4594\n",
      "Epoch [83 /100], Loss: 0.4590\n",
      "Epoch [84 /100], Loss: 0.4587\n",
      "Epoch [85 /100], Loss: 0.4583\n",
      "Epoch [86 /100], Loss: 0.4580\n",
      "Epoch [87 /100], Loss: 0.4577\n",
      "Epoch [88 /100], Loss: 0.4574\n",
      "Epoch [89 /100], Loss: 0.4571\n",
      "Epoch [90 /100], Loss: 0.4569\n",
      "Epoch [91 /100], Loss: 0.4566\n",
      "Epoch [92 /100], Loss: 0.4563\n",
      "Epoch [93 /100], Loss: 0.4561\n",
      "Epoch [94 /100], Loss: 0.4558\n",
      "Epoch [95 /100], Loss: 0.4556\n",
      "Epoch [96 /100], Loss: 0.4553\n",
      "Epoch [97 /100], Loss: 0.4551\n",
      "Epoch [98 /100], Loss: 0.4549\n",
      "Epoch [99 /100], Loss: 0.4546\n",
      "Epoch [100 /100], Loss: 0.4544\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(feature_tensor)\n",
    "    loss = criterion(outputs.squeeze(), labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1} /100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = extract_image_features(test_loader, resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor_test = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
    "labels_tensor_test = torch.tensor(y_test_encoder, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_ouptputs = model(feature_tensor_test)\n",
    "    test_predictions = torch.sigmoid(test_ouptputs).cpu().numpy()\n",
    "    test_predictions = (test_predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Напомню взял только 10 процентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(\u001b[43my_test_encoder\u001b[49m, test_features, target_names\u001b[38;5;241m=\u001b[39mmlb\u001b[38;5;241m.\u001b[39mclasses_))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoder, test_features, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
