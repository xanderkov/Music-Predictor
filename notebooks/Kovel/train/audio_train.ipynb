{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"luli0034/music-tags-to-spectrogram\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 1543\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "subset_size = int(len(ds))\n",
    "subset = ds.select(range(subset_size))\n",
    "print(subset)\n",
    "ds = subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, ds, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_frame = ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            genres = self.data_frame[index][\"text\"]\n",
    "            if self.transform:\n",
    "                image = self.transform(self.data_frame[index][\"image\"])\n",
    "            return image, genres\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return self.transform(np.ones((IMAGE_SIZE, IMAGE_SIZE, 3)), \"rock\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_collate(batch):\n",
    "#     batch = list(filter(lambda x: x is not None, batch))\n",
    "#     return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(dataloader, model):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            features.append(output.cpu().numpy())\n",
    "    return np.vstack(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поэтому я превращу их в квадрат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MusicDataset(ds_train, transform=image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MusicDataset(ds_test, transform=image_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Identity()\n",
    "model.to(device) \n",
    "resnet = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = ds_train.remove_columns('image')\n",
    "all_genres_test = ds_test.remove_columns('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = [genre[\"text\"].split(\" \") for genre in all_genres]\n",
    "all_genres_test = [genre[\"text\"].split(\" \")  for genre in all_genres_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soundtrack', 'electronic', 'experimental']"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genres[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['orchestral', 'classical', 'soundtrack']"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genres_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['pipeorgan', 'rocknroll'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(all_genres)\n",
    "y_test_encoder = mlb.transform(all_genres_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['60s', '70s', '80s', '90s', 'accordion', 'acidjazz',\n",
       "       'acousticbassguitar', 'acousticguitar', 'action', 'adventure',\n",
       "       'advertising', 'african', 'alternative', 'alternativerock',\n",
       "       'ambient', 'ambiental', 'atmospheric', 'background', 'ballad',\n",
       "       'bass', 'beat', 'bell', 'blues', 'bongo', 'bossanova', 'brass',\n",
       "       'breakbeat', 'calm', 'cello', 'celtic', 'chanson', 'children',\n",
       "       'chillout', 'choir', 'christmas', 'clarinet', 'classical',\n",
       "       'classicalguitar', 'club', 'commercial', 'computer',\n",
       "       'contemporary', 'cool', 'corporate', 'country', 'dance', 'dark',\n",
       "       'darkambient', 'darkwave', 'deep', 'deephouse', 'documentary',\n",
       "       'doublebass', 'downtempo', 'drama', 'dramatic', 'dream',\n",
       "       'drummachine', 'drumnbass', 'drums', 'dubstep', 'easylistening',\n",
       "       'edm', 'electricguitar', 'electricpiano', 'electronic',\n",
       "       'electronica', 'electropop', 'emotional', 'energetic', 'epic',\n",
       "       'ethno', 'eurodance', 'experimental', 'fast', 'film', 'flute',\n",
       "       'folk', 'fun', 'funk', 'funny', 'fusion', 'game', 'gothic',\n",
       "       'groovy', 'grunge', 'guitar', 'happy', 'hard', 'hardrock',\n",
       "       'harmonica', 'harp', 'heavy', 'hiphop', 'holiday', 'hopeful',\n",
       "       'horn', 'house', 'improvisation', 'indie', 'industrial',\n",
       "       'inspiring', 'instrumentalpop', 'instrumentalrock', 'jazz',\n",
       "       'jazzfunk', 'keyboard', 'latin', 'lounge', 'love', 'medieval',\n",
       "       'meditative', 'melancholic', 'mellow', 'melodic', 'metal',\n",
       "       'minimal', 'motivational', 'movie', 'nature', 'newage', 'newwave',\n",
       "       'oboe', 'orchestra', 'orchestral', 'oriental', 'pad', 'party',\n",
       "       'percussion', 'piano', 'pop', 'popfolk', 'poprock', 'positive',\n",
       "       'postrock', 'powerful', 'progressive', 'psychedelic', 'punkrock',\n",
       "       'rap', 'reggae', 'relaxing', 'retro', 'rhodes', 'rnb', 'rock',\n",
       "       'romantic', 'sad', 'sampler', 'saxophone', 'sexy',\n",
       "       'singersongwriter', 'ska', 'slow', 'soft', 'soul', 'soundscape',\n",
       "       'soundtrack', 'space', 'sport', 'strings', 'summer', 'swing',\n",
       "       'symphonic', 'synthesizer', 'synthpop', 'techno', 'trailer',\n",
       "       'trance', 'travel', 'tribal', 'triphop', 'trombone', 'trumpet',\n",
       "       'ukulele', 'upbeat', 'uplifting', 'viola', 'violin', 'voice',\n",
       "       'world', 'worldfusion'], dtype=object)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrecognized data stream contents when reading image file\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 197 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[392], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[379], line 5\u001b[0m, in \u001b[0;36mextract_image_features\u001b[0;34m(dataloader, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtransposed\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 197 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "train_features = extract_image_features(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self.data_frame[index][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor = torch.tensor(train_features, dtype=torch.float32).to(device)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_size=feature_tensor.shape[1], num_classes=labels_tensor.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline CV Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 /100], Loss: 0.6898\n",
      "Epoch [2 /100], Loss: 0.6828\n",
      "Epoch [3 /100], Loss: 0.6759\n",
      "Epoch [4 /100], Loss: 0.6692\n",
      "Epoch [5 /100], Loss: 0.6626\n",
      "Epoch [6 /100], Loss: 0.6560\n",
      "Epoch [7 /100], Loss: 0.6496\n",
      "Epoch [8 /100], Loss: 0.6432\n",
      "Epoch [9 /100], Loss: 0.6370\n",
      "Epoch [10 /100], Loss: 0.6308\n",
      "Epoch [11 /100], Loss: 0.6247\n",
      "Epoch [12 /100], Loss: 0.6188\n",
      "Epoch [13 /100], Loss: 0.6129\n",
      "Epoch [14 /100], Loss: 0.6071\n",
      "Epoch [15 /100], Loss: 0.6014\n",
      "Epoch [16 /100], Loss: 0.5958\n",
      "Epoch [17 /100], Loss: 0.5902\n",
      "Epoch [18 /100], Loss: 0.5848\n",
      "Epoch [19 /100], Loss: 0.5794\n",
      "Epoch [20 /100], Loss: 0.5741\n",
      "Epoch [21 /100], Loss: 0.5689\n",
      "Epoch [22 /100], Loss: 0.5638\n",
      "Epoch [23 /100], Loss: 0.5587\n",
      "Epoch [24 /100], Loss: 0.5537\n",
      "Epoch [25 /100], Loss: 0.5488\n",
      "Epoch [26 /100], Loss: 0.5440\n",
      "Epoch [27 /100], Loss: 0.5392\n",
      "Epoch [28 /100], Loss: 0.5346\n",
      "Epoch [29 /100], Loss: 0.5299\n",
      "Epoch [30 /100], Loss: 0.5254\n",
      "Epoch [31 /100], Loss: 0.5209\n",
      "Epoch [32 /100], Loss: 0.5165\n",
      "Epoch [33 /100], Loss: 0.5121\n",
      "Epoch [34 /100], Loss: 0.5078\n",
      "Epoch [35 /100], Loss: 0.5036\n",
      "Epoch [36 /100], Loss: 0.4994\n",
      "Epoch [37 /100], Loss: 0.4953\n",
      "Epoch [38 /100], Loss: 0.4913\n",
      "Epoch [39 /100], Loss: 0.4873\n",
      "Epoch [40 /100], Loss: 0.4834\n",
      "Epoch [41 /100], Loss: 0.4795\n",
      "Epoch [42 /100], Loss: 0.4757\n",
      "Epoch [43 /100], Loss: 0.4719\n",
      "Epoch [44 /100], Loss: 0.4682\n",
      "Epoch [45 /100], Loss: 0.4646\n",
      "Epoch [46 /100], Loss: 0.4610\n",
      "Epoch [47 /100], Loss: 0.4574\n",
      "Epoch [48 /100], Loss: 0.4539\n",
      "Epoch [49 /100], Loss: 0.4505\n",
      "Epoch [50 /100], Loss: 0.4470\n",
      "Epoch [51 /100], Loss: 0.4437\n",
      "Epoch [52 /100], Loss: 0.4404\n",
      "Epoch [53 /100], Loss: 0.4371\n",
      "Epoch [54 /100], Loss: 0.4339\n",
      "Epoch [55 /100], Loss: 0.4307\n",
      "Epoch [56 /100], Loss: 0.4276\n",
      "Epoch [57 /100], Loss: 0.4245\n",
      "Epoch [58 /100], Loss: 0.4215\n",
      "Epoch [59 /100], Loss: 0.4185\n",
      "Epoch [60 /100], Loss: 0.4155\n",
      "Epoch [61 /100], Loss: 0.4126\n",
      "Epoch [62 /100], Loss: 0.4097\n",
      "Epoch [63 /100], Loss: 0.4069\n",
      "Epoch [64 /100], Loss: 0.4040\n",
      "Epoch [65 /100], Loss: 0.4013\n",
      "Epoch [66 /100], Loss: 0.3986\n",
      "Epoch [67 /100], Loss: 0.3959\n",
      "Epoch [68 /100], Loss: 0.3932\n",
      "Epoch [69 /100], Loss: 0.3906\n",
      "Epoch [70 /100], Loss: 0.3880\n",
      "Epoch [71 /100], Loss: 0.3854\n",
      "Epoch [72 /100], Loss: 0.3829\n",
      "Epoch [73 /100], Loss: 0.3804\n",
      "Epoch [74 /100], Loss: 0.3780\n",
      "Epoch [75 /100], Loss: 0.3755\n",
      "Epoch [76 /100], Loss: 0.3731\n",
      "Epoch [77 /100], Loss: 0.3708\n",
      "Epoch [78 /100], Loss: 0.3685\n",
      "Epoch [79 /100], Loss: 0.3662\n",
      "Epoch [80 /100], Loss: 0.3639\n",
      "Epoch [81 /100], Loss: 0.3616\n",
      "Epoch [82 /100], Loss: 0.3594\n",
      "Epoch [83 /100], Loss: 0.3572\n",
      "Epoch [84 /100], Loss: 0.3551\n",
      "Epoch [85 /100], Loss: 0.3529\n",
      "Epoch [86 /100], Loss: 0.3508\n",
      "Epoch [87 /100], Loss: 0.3487\n",
      "Epoch [88 /100], Loss: 0.3467\n",
      "Epoch [89 /100], Loss: 0.3447\n",
      "Epoch [90 /100], Loss: 0.3427\n",
      "Epoch [91 /100], Loss: 0.3407\n",
      "Epoch [92 /100], Loss: 0.3387\n",
      "Epoch [93 /100], Loss: 0.3368\n",
      "Epoch [94 /100], Loss: 0.3349\n",
      "Epoch [95 /100], Loss: 0.3330\n",
      "Epoch [96 /100], Loss: 0.3311\n",
      "Epoch [97 /100], Loss: 0.3293\n",
      "Epoch [98 /100], Loss: 0.3275\n",
      "Epoch [99 /100], Loss: 0.3257\n",
      "Epoch [100 /100], Loss: 0.3239\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(feature_tensor)\n",
    "    loss = criterion(outputs, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1} /100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = extract_image_features(test_loader, resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor_test = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
    "labels_tensor_test = torch.tensor(y_test_encoder, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_ouptputs = model(feature_tensor_test)\n",
    "    test_predictions = torch.sigmoid(test_ouptputs).cpu().numpy()\n",
    "    test_predictions = (test_predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "               60s       0.00      0.00      0.00         0\n",
      "               70s       0.00      0.00      0.00         0\n",
      "               80s       0.00      0.00      0.00         1\n",
      "               90s       0.00      0.00      0.00         1\n",
      "         accordion       0.00      0.00      0.00         0\n",
      "          acidjazz       0.00      0.00      0.00         1\n",
      "acousticbassguitar       0.00      0.00      0.00         0\n",
      "    acousticguitar       0.00      0.00      0.00         2\n",
      "           african       0.00      0.00      0.00         0\n",
      "       alternative       0.00      0.00      0.00         5\n",
      "   alternativerock       0.00      0.00      0.00         0\n",
      "           ambient       0.00      0.00      0.00        19\n",
      "       atmospheric       0.00      0.00      0.00         7\n",
      "              bass       0.00      0.00      0.00         2\n",
      "              beat       0.00      0.00      0.00         2\n",
      "             blues       0.00      0.00      0.00         2\n",
      "         bossanova       0.00      0.00      0.00         1\n",
      "             brass       0.00      0.00      0.00         0\n",
      "         breakbeat       0.00      0.00      0.00         1\n",
      "             cello       0.00      0.00      0.00         3\n",
      "            celtic       0.00      0.00      0.00         0\n",
      "           chanson       0.00      0.00      0.00         1\n",
      "          chillout       0.00      0.00      0.00        13\n",
      "             choir       0.00      0.00      0.00         0\n",
      "          clarinet       0.00      0.00      0.00         1\n",
      "         classical       0.00      0.00      0.00        25\n",
      "   classicalguitar       0.00      0.00      0.00         0\n",
      "              club       0.00      0.00      0.00         6\n",
      "          computer       0.00      0.00      0.00         0\n",
      "      contemporary       0.00      0.00      0.00         0\n",
      "           country       0.00      0.00      0.00         0\n",
      "             dance       0.00      0.00      0.00        11\n",
      "       darkambient       0.00      0.00      0.00         2\n",
      "         deephouse       0.00      0.00      0.00         1\n",
      "        doublebass       0.00      0.00      0.00         0\n",
      "         downtempo       0.00      0.00      0.00         2\n",
      "       drummachine       0.00      0.00      0.00         3\n",
      "         drumnbass       0.00      0.00      0.00         0\n",
      "             drums       0.00      0.00      0.00         3\n",
      "           dubstep       0.00      0.00      0.00         1\n",
      "     easylistening       0.00      0.00      0.00        19\n",
      "               edm       0.00      0.00      0.00         3\n",
      "    electricguitar       0.00      0.00      0.00         8\n",
      "     electricpiano       0.00      0.00      0.00         1\n",
      "        electronic       0.00      0.00      0.00        28\n",
      "       electronica       0.00      0.00      0.00         1\n",
      "        electropop       0.00      0.00      0.00         1\n",
      "             ethno       0.00      0.00      0.00         0\n",
      "         eurodance       0.00      0.00      0.00         0\n",
      "      experimental       0.00      0.00      0.00         3\n",
      "             flute       0.00      0.00      0.00         0\n",
      "              folk       0.00      0.00      0.00         2\n",
      "              funk       0.00      0.00      0.00         4\n",
      "            fusion       0.00      0.00      0.00         1\n",
      "            gothic       0.00      0.00      0.00         1\n",
      "            grunge       0.00      0.00      0.00         1\n",
      "            guitar       0.00      0.00      0.00         5\n",
      "              hard       0.00      0.00      0.00         0\n",
      "          hardrock       0.00      0.00      0.00         0\n",
      "              harp       0.00      0.00      0.00         0\n",
      "            hiphop       0.00      0.00      0.00         4\n",
      "             house       0.00      0.00      0.00         6\n",
      "     improvisation       0.00      0.00      0.00         1\n",
      "             indie       0.00      0.00      0.00         7\n",
      "        industrial       0.00      0.00      0.00         3\n",
      "   instrumentalpop       0.00      0.00      0.00         1\n",
      "  instrumentalrock       0.00      0.00      0.00         0\n",
      "              jazz       0.00      0.00      0.00         2\n",
      "          jazzfunk       0.00      0.00      0.00         0\n",
      "          keyboard       0.00      0.00      0.00         2\n",
      "             latin       0.00      0.00      0.00         1\n",
      "            lounge       0.00      0.00      0.00         5\n",
      "          medieval       0.00      0.00      0.00         0\n",
      "             metal       0.00      0.00      0.00         3\n",
      "           minimal       0.00      0.00      0.00         2\n",
      "            newage       0.00      0.00      0.00        10\n",
      "           newwave       0.00      0.00      0.00         0\n",
      "         orchestra       0.00      0.00      0.00         0\n",
      "        orchestral       0.00      0.00      0.00        14\n",
      "               pad       0.00      0.00      0.00         0\n",
      "        percussion       0.00      0.00      0.00         0\n",
      "             piano       0.00      0.00      0.00         9\n",
      "         pipeorgan       0.00      0.00      0.00         0\n",
      "               pop       0.00      0.00      0.00        11\n",
      "           popfolk       0.00      0.00      0.00         2\n",
      "           poprock       0.00      0.00      0.00         5\n",
      "          postrock       0.00      0.00      0.00         1\n",
      "       progressive       0.00      0.00      0.00         2\n",
      "       psychedelic       0.00      0.00      0.00         2\n",
      "          punkrock       0.00      0.00      0.00         3\n",
      "               rap       0.00      0.00      0.00         1\n",
      "            reggae       0.00      0.00      0.00         1\n",
      "            rhodes       0.00      0.00      0.00         1\n",
      "               rnb       0.00      0.00      0.00         0\n",
      "              rock       0.00      0.00      0.00        10\n",
      "         rocknroll       0.00      0.00      0.00         0\n",
      "           sampler       0.00      0.00      0.00         2\n",
      "         saxophone       0.00      0.00      0.00         1\n",
      "  singersongwriter       0.00      0.00      0.00         1\n",
      "               ska       0.00      0.00      0.00         1\n",
      "              soul       0.00      0.00      0.00         1\n",
      "        soundtrack       0.00      0.00      0.00        33\n",
      "           strings       0.00      0.00      0.00         2\n",
      "             swing       0.00      0.00      0.00         0\n",
      "         symphonic       0.00      0.00      0.00         6\n",
      "       synthesizer       0.00      0.00      0.00        18\n",
      "          synthpop       0.00      0.00      0.00         3\n",
      "            techno       0.00      0.00      0.00         4\n",
      "            trance       0.00      0.00      0.00         3\n",
      "            tribal       0.00      0.00      0.00         0\n",
      "           triphop       0.00      0.00      0.00         2\n",
      "          trombone       0.00      0.00      0.00         1\n",
      "           trumpet       0.00      0.00      0.00         1\n",
      "           ukulele       0.00      0.00      0.00         0\n",
      "             viola       0.00      0.00      0.00         2\n",
      "            violin       0.00      0.00      0.00         4\n",
      "             voice       0.00      0.00      0.00         1\n",
      "             world       0.00      0.00      0.00         7\n",
      "       worldfusion       0.00      0.00      0.00         0\n",
      "\n",
      "         micro avg       0.00      0.00      0.00       396\n",
      "         macro avg       0.00      0.00      0.00       396\n",
      "      weighted avg       0.00      0.00      0.00       396\n",
      "       samples avg       0.00      0.00      0.00       396\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoder, test_predictions, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"multiclass_model_simple.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
