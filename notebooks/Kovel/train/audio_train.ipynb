{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"luli0034/music-tags-to-spectrogram\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 512\n",
    "MY_PWD = \"/Users/akovel/Documents/HSE/Music-Predictor/\"\n",
    "DS_PATH = f\"{MY_PWD}data/spectograms\"\n",
    "MIN_NUM_GENRES = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 1543\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# subset_size = int(len(ds))\n",
    "# subset = ds.select(range(subset_size))\n",
    "# print(subset)\n",
    "# ds = subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset_from_json(path):\n",
    "    path += \"/spectogramsgenres.json\"\n",
    "    df = pd.read_json(path)\n",
    "\n",
    "    return df.T\n",
    "\n",
    "df = read_dataset_from_json(DS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_simple_genres(df):\n",
    "    genre_counts = df['genres'].str.split(expand=True).stack().value_counts()\n",
    "    rare_genres = genre_counts[genre_counts < MIN_NUM_GENRES].index\n",
    "\n",
    "    def transform_genres_to_simple(genres_text):\n",
    "        genres = genres_text.split()\n",
    "        return ' '.join([genre for genre in genres if genre not in rare_genres])\n",
    "    df['simple_genre'] = df['genres'].apply(transform_genres_to_simple)\n",
    "    df = df[df['simple_genre'] != \"\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_simple_genres(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1001 entries, 0 to 1538\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   genres        1001 non-null   object\n",
      " 1   image_path    1001 non-null   object\n",
      " 2   simple_genre  1001 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 31.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genres</th>\n",
       "      <th>image_path</th>\n",
       "      <th>simple_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>soundtrack classical</td>\n",
       "      <td>/Users/akovel/Documents/HSE/Music-Predictor/da...</td>\n",
       "      <td>soundtrack classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hiphop electronic latin</td>\n",
       "      <td>/Users/akovel/Documents/HSE/Music-Predictor/da...</td>\n",
       "      <td>electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>soundtrack ambient classical</td>\n",
       "      <td>/Users/akovel/Documents/HSE/Music-Predictor/da...</td>\n",
       "      <td>soundtrack ambient classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soundtrack ambient classical</td>\n",
       "      <td>/Users/akovel/Documents/HSE/Music-Predictor/da...</td>\n",
       "      <td>soundtrack ambient classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>soundtrack ambient classical</td>\n",
       "      <td>/Users/akovel/Documents/HSE/Music-Predictor/da...</td>\n",
       "      <td>soundtrack ambient classical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         genres  \\\n",
       "0          soundtrack classical   \n",
       "1       hiphop electronic latin   \n",
       "3  soundtrack ambient classical   \n",
       "4  soundtrack ambient classical   \n",
       "5  soundtrack ambient classical   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  /Users/akovel/Documents/HSE/Music-Predictor/da...   \n",
       "1  /Users/akovel/Documents/HSE/Music-Predictor/da...   \n",
       "3  /Users/akovel/Documents/HSE/Music-Predictor/da...   \n",
       "4  /Users/akovel/Documents/HSE/Music-Predictor/da...   \n",
       "5  /Users/akovel/Documents/HSE/Music-Predictor/da...   \n",
       "\n",
       "                   simple_genre  \n",
       "0          soundtrack classical  \n",
       "1                    electronic  \n",
       "3  soundtrack ambient classical  \n",
       "4  soundtrack ambient classical  \n",
       "5  soundtrack ambient classical  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, ds, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_frame = ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        genres = self.data_frame[\"simple_genre\"].iloc[index]\n",
    "        if self.transform:\n",
    "            image = Image.open(self.data_frame[\"image_path\"].iloc[index])\n",
    "            image = self.transform(image)\n",
    "        return image, genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_collate(batch):\n",
    "#     batch = list(filter(lambda x: x is not None, batch))\n",
    "#     return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(dataloader, model):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in tqdm(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            features.append(output.cpu().numpy())\n",
    "    return np.vstack(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поэтому я превращу их в квадрат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MusicDataset(ds_train, transform=image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MusicDataset(ds_test, transform=image_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akovel/anaconda3/envs/diploma/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/akovel/anaconda3/envs/diploma/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Identity()\n",
    "model.to(device) \n",
    "resnet = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soundtrack', 'classical']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[\"simple_genre\"][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = [genre.split() for genre in ds_train[\"simple_genre\"]]\n",
    "all_genres_test = [genre.split()  for genre in ds_test[\"simple_genre\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['piano']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genres[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pop']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_genres_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(all_genres)\n",
    "y_test_encoder = mlb.transform(all_genres_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ambient', 'bass', 'chillout', 'classical', 'dance', 'drums',\n",
       "       'easylistening', 'electricguitar', 'electronic', 'emotional',\n",
       "       'film', 'guitar', 'happy', 'newage', 'orchestral', 'piano', 'pop',\n",
       "       'relaxing', 'rock', 'soundtrack', 'synthesizer'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mlb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [09:30<00:00, 43.89s/it]\n"
     ]
    }
   ],
   "source": [
    "train_features = extract_image_features(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor = torch.tensor(train_features, dtype=torch.float32).to(device)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_size=feature_tensor.shape[1], num_classes=labels_tensor.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline CV Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 /100], Loss: 0.7006\n",
      "Epoch [2 /100], Loss: 0.6761\n",
      "Epoch [3 /100], Loss: 0.6533\n",
      "Epoch [4 /100], Loss: 0.6320\n",
      "Epoch [5 /100], Loss: 0.6120\n",
      "Epoch [6 /100], Loss: 0.5934\n",
      "Epoch [7 /100], Loss: 0.5760\n",
      "Epoch [8 /100], Loss: 0.5598\n",
      "Epoch [9 /100], Loss: 0.5445\n",
      "Epoch [10 /100], Loss: 0.5303\n",
      "Epoch [11 /100], Loss: 0.5170\n",
      "Epoch [12 /100], Loss: 0.5045\n",
      "Epoch [13 /100], Loss: 0.4927\n",
      "Epoch [14 /100], Loss: 0.4817\n",
      "Epoch [15 /100], Loss: 0.4714\n",
      "Epoch [16 /100], Loss: 0.4617\n",
      "Epoch [17 /100], Loss: 0.4526\n",
      "Epoch [18 /100], Loss: 0.4440\n",
      "Epoch [19 /100], Loss: 0.4360\n",
      "Epoch [20 /100], Loss: 0.4283\n",
      "Epoch [21 /100], Loss: 0.4212\n",
      "Epoch [22 /100], Loss: 0.4144\n",
      "Epoch [23 /100], Loss: 0.4080\n",
      "Epoch [24 /100], Loss: 0.4019\n",
      "Epoch [25 /100], Loss: 0.3962\n",
      "Epoch [26 /100], Loss: 0.3908\n",
      "Epoch [27 /100], Loss: 0.3857\n",
      "Epoch [28 /100], Loss: 0.3808\n",
      "Epoch [29 /100], Loss: 0.3762\n",
      "Epoch [30 /100], Loss: 0.3718\n",
      "Epoch [31 /100], Loss: 0.3677\n",
      "Epoch [32 /100], Loss: 0.3637\n",
      "Epoch [33 /100], Loss: 0.3599\n",
      "Epoch [34 /100], Loss: 0.3564\n",
      "Epoch [35 /100], Loss: 0.3530\n",
      "Epoch [36 /100], Loss: 0.3497\n",
      "Epoch [37 /100], Loss: 0.3466\n",
      "Epoch [38 /100], Loss: 0.3437\n",
      "Epoch [39 /100], Loss: 0.3408\n",
      "Epoch [40 /100], Loss: 0.3382\n",
      "Epoch [41 /100], Loss: 0.3356\n",
      "Epoch [42 /100], Loss: 0.3331\n",
      "Epoch [43 /100], Loss: 0.3308\n",
      "Epoch [44 /100], Loss: 0.3285\n",
      "Epoch [45 /100], Loss: 0.3264\n",
      "Epoch [46 /100], Loss: 0.3243\n",
      "Epoch [47 /100], Loss: 0.3223\n",
      "Epoch [48 /100], Loss: 0.3204\n",
      "Epoch [49 /100], Loss: 0.3186\n",
      "Epoch [50 /100], Loss: 0.3168\n",
      "Epoch [51 /100], Loss: 0.3152\n",
      "Epoch [52 /100], Loss: 0.3135\n",
      "Epoch [53 /100], Loss: 0.3120\n",
      "Epoch [54 /100], Loss: 0.3105\n",
      "Epoch [55 /100], Loss: 0.3091\n",
      "Epoch [56 /100], Loss: 0.3077\n",
      "Epoch [57 /100], Loss: 0.3064\n",
      "Epoch [58 /100], Loss: 0.3051\n",
      "Epoch [59 /100], Loss: 0.3039\n",
      "Epoch [60 /100], Loss: 0.3027\n",
      "Epoch [61 /100], Loss: 0.3015\n",
      "Epoch [62 /100], Loss: 0.3004\n",
      "Epoch [63 /100], Loss: 0.2994\n",
      "Epoch [64 /100], Loss: 0.2983\n",
      "Epoch [65 /100], Loss: 0.2973\n",
      "Epoch [66 /100], Loss: 0.2964\n",
      "Epoch [67 /100], Loss: 0.2955\n",
      "Epoch [68 /100], Loss: 0.2946\n",
      "Epoch [69 /100], Loss: 0.2937\n",
      "Epoch [70 /100], Loss: 0.2929\n",
      "Epoch [71 /100], Loss: 0.2921\n",
      "Epoch [72 /100], Loss: 0.2913\n",
      "Epoch [73 /100], Loss: 0.2905\n",
      "Epoch [74 /100], Loss: 0.2898\n",
      "Epoch [75 /100], Loss: 0.2891\n",
      "Epoch [76 /100], Loss: 0.2884\n",
      "Epoch [77 /100], Loss: 0.2878\n",
      "Epoch [78 /100], Loss: 0.2871\n",
      "Epoch [79 /100], Loss: 0.2865\n",
      "Epoch [80 /100], Loss: 0.2859\n",
      "Epoch [81 /100], Loss: 0.2853\n",
      "Epoch [82 /100], Loss: 0.2848\n",
      "Epoch [83 /100], Loss: 0.2842\n",
      "Epoch [84 /100], Loss: 0.2837\n",
      "Epoch [85 /100], Loss: 0.2832\n",
      "Epoch [86 /100], Loss: 0.2827\n",
      "Epoch [87 /100], Loss: 0.2822\n",
      "Epoch [88 /100], Loss: 0.2817\n",
      "Epoch [89 /100], Loss: 0.2813\n",
      "Epoch [90 /100], Loss: 0.2808\n",
      "Epoch [91 /100], Loss: 0.2804\n",
      "Epoch [92 /100], Loss: 0.2800\n",
      "Epoch [93 /100], Loss: 0.2796\n",
      "Epoch [94 /100], Loss: 0.2792\n",
      "Epoch [95 /100], Loss: 0.2788\n",
      "Epoch [96 /100], Loss: 0.2784\n",
      "Epoch [97 /100], Loss: 0.2780\n",
      "Epoch [98 /100], Loss: 0.2777\n",
      "Epoch [99 /100], Loss: 0.2773\n",
      "Epoch [100 /100], Loss: 0.2770\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(feature_tensor)\n",
    "    loss = criterion(outputs, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1} /100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:11<00:00, 32.98s/it]\n"
     ]
    }
   ],
   "source": [
    "test_features = extract_image_features(test_loader, resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor_test = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
    "labels_tensor_test = torch.tensor(y_test_encoder, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_ouptputs = model(feature_tensor_test)\n",
    "    test_predictions = torch.sigmoid(test_ouptputs).cpu().numpy()\n",
    "    test_predictions = (test_predictions > 0.1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "       ambient       0.10      0.95      0.17        20\n",
      "          bass       0.04      0.64      0.08        11\n",
      "      chillout       0.04      0.88      0.08         8\n",
      "     classical       0.09      1.00      0.16        17\n",
      "         dance       0.05      1.00      0.09         6\n",
      "         drums       0.07      1.00      0.13        13\n",
      " easylistening       0.07      0.93      0.13        14\n",
      "electricguitar       0.03      0.67      0.07         6\n",
      "    electronic       0.12      1.00      0.21        24\n",
      "     emotional       0.06      0.75      0.11         8\n",
      "          film       0.06      0.67      0.11        12\n",
      "        guitar       0.06      1.00      0.11         9\n",
      "         happy       0.06      0.82      0.12        11\n",
      "        newage       0.04      0.86      0.09         7\n",
      "    orchestral       0.03      0.71      0.06         7\n",
      "         piano       0.15      1.00      0.26        30\n",
      "           pop       0.06      0.92      0.12        13\n",
      "      relaxing       0.04      0.40      0.08        10\n",
      "          rock       0.05      0.67      0.10        12\n",
      "    soundtrack       0.16      1.00      0.28        33\n",
      "   synthesizer       0.12      1.00      0.21        24\n",
      "\n",
      "     micro avg       0.08      0.89      0.14       295\n",
      "     macro avg       0.07      0.85      0.13       295\n",
      "  weighted avg       0.09      0.89      0.16       295\n",
      "   samples avg       0.08      0.88      0.14       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoder, test_predictions, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"multiclass_model_simple.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
