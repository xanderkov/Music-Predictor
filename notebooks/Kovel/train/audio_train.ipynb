{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"luli0034/music-tags-to-spectrogram\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataloader import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 154\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "subset_size = int(len(ds) * 0.5)\n",
    "subset = ds.select(range(subset_size))\n",
    "print(subset)\n",
    "ds = subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test = ds[\"train\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, ds, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data_frame = ds\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            genres = self.data_frame[index][\"text\"]\n",
    "            if self.transform:\n",
    "                image = self.transform(self.data_frame[index][\"image\"])\n",
    "            return image, genres\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return np.ones((256, 256, 3)), self.data_frame[index][\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_collate(batch):\n",
    "#     batch = list(filter(lambda x: x is not None, batch))\n",
    "#     return default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(dataloader, model):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            features.append(output.cpu().numpy())\n",
    "    return np.vstack(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поэтому я превращу их в квадрат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MusicDataset(ds_train, transform=image_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MusicDataset(ds_test, transform=image_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Identity()\n",
    "model.to(device) \n",
    "resnet = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = ds_train.remove_columns('image')\n",
    "all_genres_test = ds_test.remove_columns('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = [genre[\"text\"] for genre in all_genres]\n",
    "all_genres_test = [genre[\"text\"] for genre in all_genres_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "y_train = mlb.fit_transform(all_genres)\n",
    "y_test_encoder = mlb.transform(all_genres_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0],\n",
       "       [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1],\n",
       "       [1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 1, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = extract_image_features(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor = torch.tensor(train_features, dtype=torch.float32).to(device)\n",
    "labels_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(input_size=feature_tensor.shape[1], num_classes=labels_tensor.shape[1]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline CV Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 /100], Loss: 0.7156\n",
      "Epoch [2 /100], Loss: 0.7018\n",
      "Epoch [3 /100], Loss: 0.6889\n",
      "Epoch [4 /100], Loss: 0.6768\n",
      "Epoch [5 /100], Loss: 0.6656\n",
      "Epoch [6 /100], Loss: 0.6551\n",
      "Epoch [7 /100], Loss: 0.6453\n",
      "Epoch [8 /100], Loss: 0.6362\n",
      "Epoch [9 /100], Loss: 0.6277\n",
      "Epoch [10 /100], Loss: 0.6197\n",
      "Epoch [11 /100], Loss: 0.6123\n",
      "Epoch [12 /100], Loss: 0.6053\n",
      "Epoch [13 /100], Loss: 0.5988\n",
      "Epoch [14 /100], Loss: 0.5928\n",
      "Epoch [15 /100], Loss: 0.5871\n",
      "Epoch [16 /100], Loss: 0.5817\n",
      "Epoch [17 /100], Loss: 0.5768\n",
      "Epoch [18 /100], Loss: 0.5721\n",
      "Epoch [19 /100], Loss: 0.5677\n",
      "Epoch [20 /100], Loss: 0.5635\n",
      "Epoch [21 /100], Loss: 0.5597\n",
      "Epoch [22 /100], Loss: 0.5560\n",
      "Epoch [23 /100], Loss: 0.5526\n",
      "Epoch [24 /100], Loss: 0.5494\n",
      "Epoch [25 /100], Loss: 0.5463\n",
      "Epoch [26 /100], Loss: 0.5435\n",
      "Epoch [27 /100], Loss: 0.5408\n",
      "Epoch [28 /100], Loss: 0.5382\n",
      "Epoch [29 /100], Loss: 0.5358\n",
      "Epoch [30 /100], Loss: 0.5335\n",
      "Epoch [31 /100], Loss: 0.5314\n",
      "Epoch [32 /100], Loss: 0.5293\n",
      "Epoch [33 /100], Loss: 0.5274\n",
      "Epoch [34 /100], Loss: 0.5256\n",
      "Epoch [35 /100], Loss: 0.5239\n",
      "Epoch [36 /100], Loss: 0.5222\n",
      "Epoch [37 /100], Loss: 0.5207\n",
      "Epoch [38 /100], Loss: 0.5192\n",
      "Epoch [39 /100], Loss: 0.5178\n",
      "Epoch [40 /100], Loss: 0.5164\n",
      "Epoch [41 /100], Loss: 0.5152\n",
      "Epoch [42 /100], Loss: 0.5140\n",
      "Epoch [43 /100], Loss: 0.5128\n",
      "Epoch [44 /100], Loss: 0.5117\n",
      "Epoch [45 /100], Loss: 0.5107\n",
      "Epoch [46 /100], Loss: 0.5097\n",
      "Epoch [47 /100], Loss: 0.5087\n",
      "Epoch [48 /100], Loss: 0.5078\n",
      "Epoch [49 /100], Loss: 0.5070\n",
      "Epoch [50 /100], Loss: 0.5061\n",
      "Epoch [51 /100], Loss: 0.5053\n",
      "Epoch [52 /100], Loss: 0.5046\n",
      "Epoch [53 /100], Loss: 0.5039\n",
      "Epoch [54 /100], Loss: 0.5032\n",
      "Epoch [55 /100], Loss: 0.5025\n",
      "Epoch [56 /100], Loss: 0.5019\n",
      "Epoch [57 /100], Loss: 0.5013\n",
      "Epoch [58 /100], Loss: 0.5007\n",
      "Epoch [59 /100], Loss: 0.5001\n",
      "Epoch [60 /100], Loss: 0.4996\n",
      "Epoch [61 /100], Loss: 0.4991\n",
      "Epoch [62 /100], Loss: 0.4986\n",
      "Epoch [63 /100], Loss: 0.4981\n",
      "Epoch [64 /100], Loss: 0.4976\n",
      "Epoch [65 /100], Loss: 0.4972\n",
      "Epoch [66 /100], Loss: 0.4968\n",
      "Epoch [67 /100], Loss: 0.4963\n",
      "Epoch [68 /100], Loss: 0.4960\n",
      "Epoch [69 /100], Loss: 0.4956\n",
      "Epoch [70 /100], Loss: 0.4952\n",
      "Epoch [71 /100], Loss: 0.4949\n",
      "Epoch [72 /100], Loss: 0.4945\n",
      "Epoch [73 /100], Loss: 0.4942\n",
      "Epoch [74 /100], Loss: 0.4939\n",
      "Epoch [75 /100], Loss: 0.4936\n",
      "Epoch [76 /100], Loss: 0.4933\n",
      "Epoch [77 /100], Loss: 0.4930\n",
      "Epoch [78 /100], Loss: 0.4927\n",
      "Epoch [79 /100], Loss: 0.4924\n",
      "Epoch [80 /100], Loss: 0.4922\n",
      "Epoch [81 /100], Loss: 0.4919\n",
      "Epoch [82 /100], Loss: 0.4917\n",
      "Epoch [83 /100], Loss: 0.4914\n",
      "Epoch [84 /100], Loss: 0.4912\n",
      "Epoch [85 /100], Loss: 0.4910\n",
      "Epoch [86 /100], Loss: 0.4908\n",
      "Epoch [87 /100], Loss: 0.4905\n",
      "Epoch [88 /100], Loss: 0.4903\n",
      "Epoch [89 /100], Loss: 0.4901\n",
      "Epoch [90 /100], Loss: 0.4900\n",
      "Epoch [91 /100], Loss: 0.4898\n",
      "Epoch [92 /100], Loss: 0.4896\n",
      "Epoch [93 /100], Loss: 0.4894\n",
      "Epoch [94 /100], Loss: 0.4892\n",
      "Epoch [95 /100], Loss: 0.4891\n",
      "Epoch [96 /100], Loss: 0.4889\n",
      "Epoch [97 /100], Loss: 0.4887\n",
      "Epoch [98 /100], Loss: 0.4886\n",
      "Epoch [99 /100], Loss: 0.4884\n",
      "Epoch [100 /100], Loss: 0.4883\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(feature_tensor)\n",
    "    loss = criterion(outputs, labels_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1} /100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = extract_image_features(test_loader, resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor_test = torch.tensor(test_features, dtype=torch.float32).to(device)\n",
    "labels_tensor_test = torch.tensor(y_test_encoder, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_ouptputs = model(feature_tensor_test)\n",
    "    test_predictions = torch.sigmoid(test_ouptputs).cpu().numpy()\n",
    "    test_predictions = (test_predictions > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "                   0.94      1.00      0.97        29\n",
      "           a       0.84      1.00      0.91        26\n",
      "           b       0.00      0.00      0.00        11\n",
      "           c       0.84      1.00      0.91        26\n",
      "           d       0.00      0.00      0.00        15\n",
      "           e       0.71      1.00      0.83        22\n",
      "           f       0.00      0.00      0.00         3\n",
      "           g       0.00      0.00      0.00         8\n",
      "           h       0.00      0.00      0.00        10\n",
      "           i       0.77      1.00      0.87        24\n",
      "           j       0.00      0.00      0.00         1\n",
      "           k       0.00      0.00      0.00        17\n",
      "           l       0.84      1.00      0.91        26\n",
      "           m       0.00      0.00      0.00         9\n",
      "           n       0.90      1.00      0.95        28\n",
      "           o       0.87      1.00      0.93        27\n",
      "           p       0.00      0.00      0.00        10\n",
      "           r       0.81      1.00      0.89        25\n",
      "           s       0.74      1.00      0.85        23\n",
      "           t       0.81      1.00      0.89        25\n",
      "           u       0.00      0.00      0.00        16\n",
      "           v       0.00      0.00      0.00         3\n",
      "           w       0.00      0.00      0.00         6\n",
      "           x       0.00      0.00      0.00         0\n",
      "           y       0.00      0.00      0.00         7\n",
      "           z       0.00      0.00      0.00         1\n",
      "\n",
      "   micro avg       0.82      0.71      0.76       398\n",
      "   macro avg       0.35      0.42      0.38       398\n",
      "weighted avg       0.59      0.71      0.64       398\n",
      " samples avg       0.82      0.72      0.75       398\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/akovel/Documents/HSE/Music-Predictor/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_encoder, test_predictions, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"multiclass_model_simple.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
